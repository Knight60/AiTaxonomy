{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Data Preparation V4 (Save Class List)\n",
    "\n",
    "**Update:** Automatically saves `classes.json` into the Run ID folder to ensure class consistency during inference.\n",
    "**Path:** /workspace/AiTaxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 14:17:45.455337: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 14:17:45.495496: I tensorflow/core/platform/cpu_feature_guard.cc:211] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration Loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pillow_heif\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "DATA_DIR = r\"/workspace/Archive/All-Species\"\n",
    "OUTPUT_BASE_DIR = r\"/workspace/AiTaxonomy/TFRecords_AllSpecies_B6\"\n",
    "LOG_DIR = r\"/workspace/AiTaxonomy/TF-Training-Logs-B6\"\n",
    "\n",
    "IMG_SIZE = 528\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 123\n",
    "IMAGES_PER_SHARD = 2000 \n",
    "# =================================================\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
    "\n",
    "def get_user_input(prompt):\n",
    "    return input(prompt).strip()\n",
    "\n",
    "def get_thai_timestamp():\n",
    "    tz_thai = timezone(timedelta(hours=7))\n",
    "    return datetime.now(tz_thai).strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(f\"‚úÖ Configuration Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Existing IDs: ['20251202-115257', '20251203-065556', '20251204-211157']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter RUN ID to resume/overwrite (or press Enter for NEW):  20251204-211157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Resume  2) Overwrite  3) Cancel\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select:  1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üìù STEP 1: Select ID\n",
    "# =============================================================================\n",
    "\n",
    "existing_ids = sorted(os.listdir(OUTPUT_BASE_DIR)) if os.path.exists(OUTPUT_BASE_DIR) else []\n",
    "print(f\"üìÇ Existing IDs: {existing_ids}\")\n",
    "\n",
    "user_id = get_user_input(\"Enter RUN ID to resume/overwrite (or press Enter for NEW): \")\n",
    "\n",
    "if not user_id:\n",
    "    RUN_TIMESTAMP = get_thai_timestamp()\n",
    "    MODE = 'NEW'\n",
    "    print(f\"‚ú® NEW ID: {RUN_TIMESTAMP}\")\n",
    "else:\n",
    "    RUN_TIMESTAMP = user_id\n",
    "    target_dir = os.path.join(OUTPUT_BASE_DIR, RUN_TIMESTAMP)\n",
    "    if os.path.exists(target_dir):\n",
    "        print(\"1) Resume  2) Overwrite  3) Cancel\")\n",
    "        choice = get_user_input(\"Select: \")\n",
    "        if choice == '1': MODE = 'RESUME'\n",
    "        elif choice == '2': MODE = 'OVERWRITE'\n",
    "        else: MODE = 'CANCEL'\n",
    "    else:\n",
    "        MODE = 'NEW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìù STEP 2: Processing Functions\n",
    "# =============================================================================\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(image_string, label):\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image_string),\n",
    "        'label': _int64_feature(label),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def process_image_safely(img_path, target_size):\n",
    "    try:\n",
    "        if not os.path.exists(img_path) or os.path.getsize(img_path) == 0: return None\n",
    "        ext = os.path.splitext(img_path)[1].lower()\n",
    "        img = None\n",
    "        if ext in ['.heic', '.heif']:\n",
    "            heif_file = pillow_heif.read_heif(img_path)\n",
    "            image = Image.frombytes(heif_file.mode, heif_file.size, heif_file.data, \"raw\")\n",
    "            img = np.array(image)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            with open(img_path, \"rb\") as stream:\n",
    "                bytes_data = bytearray(stream.read())\n",
    "                numpyarray = np.asarray(bytes_data, dtype=np.uint8)\n",
    "                img = cv2.imdecode(numpyarray, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if img is None or img.size == 0: return None\n",
    "        img = cv2.resize(img, (target_size, target_size))\n",
    "        is_success, img_encoded = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "        if not is_success: return None\n",
    "        return img_encoded.tobytes()\n",
    "    except: return None\n",
    "\n",
    "def write_tfrecords(data, output_dir, prefix, class_map, resume=False):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    start_index = 0\n",
    "    shard_idx = 0\n",
    "    \n",
    "    if resume:\n",
    "        files = sorted(glob.glob(os.path.join(output_dir, f\"{prefix}_*.tfrecord\")))\n",
    "        if files:\n",
    "            last_file = files[-1]\n",
    "            try: os.remove(last_file) \n",
    "            except: pass\n",
    "            shard_idx = len(files) - 1\n",
    "            start_index = shard_idx * IMAGES_PER_SHARD\n",
    "            print(f\"üîÑ Resuming from index {start_index} (Shard {shard_idx})\")\n",
    "\n",
    "    if start_index >= len(data):\n",
    "        print(f\"‚úÖ {prefix} already complete.\")\n",
    "        return\n",
    "\n",
    "    writer = None\n",
    "    data_to_process = data[start_index:]\n",
    "    print(f\"Processing {len(data_to_process)} images...\")\n",
    "    \n",
    "    for i, img_path in tqdm(enumerate(data_to_process), total=len(data_to_process)):\n",
    "        if writer is None or (i % IMAGES_PER_SHARD == 0):\n",
    "            if writer: writer.close()\n",
    "            shard_path = os.path.join(output_dir, f\"{prefix}_{shard_idx:04d}.tfrecord\")\n",
    "            writer = tf.io.TFRecordWriter(shard_path)\n",
    "            shard_idx += 1\n",
    "        \n",
    "        class_name = os.path.basename(os.path.dirname(img_path))\n",
    "        label = class_map.get(class_name)\n",
    "        \n",
    "        if label is not None:\n",
    "            img_bytes = process_image_safely(img_path, IMG_SIZE)\n",
    "            if img_bytes:\n",
    "                writer.write(serialize_example(img_bytes, label))\n",
    "    \n",
    "    if writer: writer.close()\n",
    "    print(f\"‚úÖ {prefix} Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning classes...\n",
      "üíæ Saved Class List to: /workspace/AiTaxonomy/TFRecords_AllSpecies_B6/20251204-211157/classes.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b679dbdd562c4d91bae15cee172739c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Valid Files: 169283 (Train: 135427, Val: 33856)\n",
      "üîÑ Resuming from index 0 (Shard 0)\n",
      "Processing 135427 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e50bcbdb8e940a5a278f5ebac4e3c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 14:18:33.393450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 90683 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:26:00.0, compute capability: 9.0\n",
      "2025-12-04 14:18:33.394841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 90683 MB memory:  -> device: 1, name: NVIDIA H100 NVL, pci bus id: 0000:8a:00.0, compute capability: 9.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üìù STEP 3: EXECUTE & SAVE CLASS MAP\n",
    "# =============================================================================\n",
    "\n",
    "if MODE != 'CANCEL':\n",
    "    SAVE_DIR = os.path.join(OUTPUT_BASE_DIR, RUN_TIMESTAMP)\n",
    "    \n",
    "    if MODE == 'OVERWRITE':\n",
    "        print(f\"üóëÔ∏è Deleting old data in {SAVE_DIR}...\")\n",
    "        shutil.rmtree(SAVE_DIR)\n",
    "    \n",
    "    # 1. Scan & Create Class Map\n",
    "    print(\"üîç Scanning classes...\")\n",
    "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "    class_map = {name: i for i, name in enumerate(classes)}\n",
    "    \n",
    "    # --- NEW: Save Class List to JSON ---\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    class_json_path = os.path.join(SAVE_DIR, \"classes.json\")\n",
    "    with open(class_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(classes, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"üíæ Saved Class List to: {class_json_path}\")\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # 2. File Scanning\n",
    "    all_files = []\n",
    "    valid_ext = {'.jpg', '.jpeg', '.png', '.bmp', '.webp', '.heic', '.heif'}\n",
    "    \n",
    "    for cls in tqdm(classes):\n",
    "        cls_path = os.path.join(DATA_DIR, cls)\n",
    "        if os.path.exists(cls_path):\n",
    "            for f in os.listdir(cls_path):\n",
    "                if os.path.splitext(f)[1].lower() in valid_ext:\n",
    "                    if 'bark01' not in f.lower():\n",
    "                        all_files.append(os.path.join(cls_path, f))\n",
    "\n",
    "    # 3. Shuffle & Split\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(all_files)\n",
    "    val_count = int(len(all_files) * VAL_SPLIT)\n",
    "    train_files = all_files[val_count:]\n",
    "    val_files = all_files[:val_count]\n",
    "    \n",
    "    print(f\"üìä Valid Files: {len(all_files)} (Train: {len(train_files)}, Val: {len(val_files)})\")\n",
    "    \n",
    "    # 4. Process\n",
    "    is_resume = (MODE == 'RESUME')\n",
    "    write_tfrecords(train_files, os.path.join(SAVE_DIR, 'train'), 'train_data', class_map, resume=is_resume)\n",
    "    write_tfrecords(val_files, os.path.join(SAVE_DIR, 'val'), 'val_data', class_map, resume=is_resume)\n",
    "    \n",
    "    print(f\"\\nüéâ DATA PREP COMPLETED. ID: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
