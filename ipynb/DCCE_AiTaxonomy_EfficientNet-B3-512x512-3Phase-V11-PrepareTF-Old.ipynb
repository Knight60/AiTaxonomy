{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Data Preparation (Final Fix: Robust Validation + Local Time)\n",
    "\n",
    "**Features:** \n",
    "* **Thai Time:** Run ID uses GMT+7 (Thailand Time).\n",
    "* **Strict Validation:** Skips corrupt/empty/unreadable images immediately.\n",
    "* **Filter:** Excludes 'bark01' images.\n",
    "* **HEIC Support:** Handles .heic files safely.\n",
    "    \n",
    "**Path:** /workspace/AiTaxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HEIC library if missing (One-time setup)\n",
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "    import pillow_heif\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pillow-heif\"])\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pillow_heif\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "DATA_DIR = r\"/workspace/Archive/All-Species\"\n",
    "OUTPUT_BASE_DIR = r\"/workspace/AiTaxonomy/TFRecords_AllSpecies_B6\"\n",
    "LOG_DIR = r\"/workspace/AiTaxonomy/TF-Training-Logs-B6\"\n",
    "\n",
    "IMG_SIZE = 528\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 123\n",
    "IMAGES_PER_SHARD = 2000 \n",
    "# =================================================\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def get_user_input(prompt):\n",
    "    return input(prompt).strip()\n",
    "\n",
    "def get_thai_timestamp():\n",
    "    # Create GMT+7 Timezone\n",
    "    tz_thai = timezone(timedelta(hours=7))\n",
    "    return datetime.now(tz_thai).strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(f\"‚úÖ Configuration Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìù STEP 1: Select ID (Thai Time)\n",
    "# =============================================================================\n",
    "\n",
    "existing_ids = sorted(os.listdir(OUTPUT_BASE_DIR)) if os.path.exists(OUTPUT_BASE_DIR) else []\n",
    "print(f\"üìÇ Existing IDs: {existing_ids}\")\n",
    "\n",
    "user_id = get_user_input(\"Enter RUN ID to resume/overwrite (or press Enter for NEW): \")\n",
    "\n",
    "if not user_id:\n",
    "    RUN_TIMESTAMP = get_thai_timestamp()\n",
    "    MODE = 'NEW'\n",
    "    print(f\"‚ú® NEW ID (Thai Time): {RUN_TIMESTAMP}\")\n",
    "else:\n",
    "    RUN_TIMESTAMP = user_id\n",
    "    target_dir = os.path.join(OUTPUT_BASE_DIR, RUN_TIMESTAMP)\n",
    "    if os.path.exists(target_dir):\n",
    "        print(\"1) Resume  2) Overwrite  3) Cancel\")\n",
    "        choice = get_user_input(\"Select: \")\n",
    "        if choice == '1': MODE = 'RESUME'\n",
    "        elif choice == '2': MODE = 'OVERWRITE'\n",
    "        else: MODE = 'CANCEL'\n",
    "    else:\n",
    "        MODE = 'NEW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìù STEP 2: Strict Processing Functions\n",
    "# =============================================================================\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(image_string, label):\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image_string),\n",
    "        'label': _int64_feature(label),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def process_image_safely(img_path, target_size):\n",
    "    \"\"\"Reads image with strict checks. Returns bytes if valid, None if corrupt.\"\"\"\n",
    "    try:\n",
    "        # 1. Check File Existence & Size\n",
    "        if not os.path.exists(img_path) or os.path.getsize(img_path) == 0:\n",
    "            print(f\"‚ùå Skipping Zero-byte/Missing file: {img_path}\")\n",
    "            return None\n",
    "\n",
    "        ext = os.path.splitext(img_path)[1].lower()\n",
    "        img = None\n",
    "        \n",
    "        # 2. Decode Image\n",
    "        if ext in ['.heic', '.heif']:\n",
    "            heif_file = pillow_heif.read_heif(img_path)\n",
    "            image = Image.frombytes(heif_file.mode, heif_file.size, heif_file.data, \"raw\")\n",
    "            img = np.array(image)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            # Use byte stream to avoid path encoding issues\n",
    "            with open(img_path, \"rb\") as stream:\n",
    "                bytes_data = bytearray(stream.read())\n",
    "                numpyarray = np.asarray(bytes_data, dtype=np.uint8)\n",
    "                img = cv2.imdecode(numpyarray, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # 3. Validate Content\n",
    "        if img is None:\n",
    "            print(f\"‚ùå Skipping Corrupt Image (Decode Failed): {img_path}\")\n",
    "            return None\n",
    "        \n",
    "        if img.size == 0 or img.shape[0] == 0 or img.shape[1] == 0:\n",
    "            print(f\"‚ùå Skipping Empty Dimensions: {img_path}\")\n",
    "            return None\n",
    "\n",
    "        # 4. Resize & Encode\n",
    "        img = cv2.resize(img, (target_size, target_size))\n",
    "        is_success, img_encoded = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "        \n",
    "        if not is_success:\n",
    "            print(f\"‚ùå Skipping Encode Error: {img_path}\")\n",
    "            return None\n",
    "            \n",
    "        return img_encoded.tobytes()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_tfrecords_robust(data, output_dir, prefix, class_map, resume=False):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    \n",
    "    start_index = 0\n",
    "    shard_idx = 0\n",
    "    \n",
    "    # Clean Resume: Delete last shard to prevent corruption\n",
    "    if resume:\n",
    "        files = sorted(glob.glob(os.path.join(output_dir, f\"{prefix}_*.tfrecord\")))\n",
    "        if files:\n",
    "            last_file = files[-1]\n",
    "            try: \n",
    "                os.remove(last_file) \n",
    "                print(f\"‚ö†Ô∏è Removed last shard {last_file} to ensure data integrity.\")\n",
    "            except: pass\n",
    "            shard_idx = len(files) - 1\n",
    "            start_index = shard_idx * IMAGES_PER_SHARD\n",
    "            print(f\"üîÑ Resuming from index {start_index} (Shard {shard_idx})\")\n",
    "\n",
    "    if start_index >= len(data):\n",
    "        print(f\"‚úÖ {prefix} already complete.\")\n",
    "        return\n",
    "\n",
    "    writer = None\n",
    "    data_to_process = data[start_index:]\n",
    "    \n",
    "    print(f\"Processing {len(data_to_process)} candidates for {output_dir}...\")\n",
    "    \n",
    "    valid_count = 0\n",
    "    \n",
    "    for i, img_path in tqdm(enumerate(data_to_process), total=len(data_to_process)):\n",
    "        # Rotate shard\n",
    "        if valid_count % IMAGES_PER_SHARD == 0 and valid_count > 0:\n",
    "            pass\n",
    "\n",
    "        # Simple Sharding Logic\n",
    "        if writer is None or (i % IMAGES_PER_SHARD == 0):\n",
    "            if writer: writer.close()\n",
    "            shard_path = os.path.join(output_dir, f\"{prefix}_{shard_idx:04d}.tfrecord\")\n",
    "            writer = tf.io.TFRecordWriter(shard_path)\n",
    "            shard_idx += 1\n",
    "        \n",
    "        class_name = os.path.basename(os.path.dirname(img_path))\n",
    "        label = class_map.get(class_name)\n",
    "        \n",
    "        if label is not None:\n",
    "            img_bytes = process_image_safely(img_path, IMG_SIZE)\n",
    "            if img_bytes:\n",
    "                writer.write(serialize_example(img_bytes, label))\n",
    "                valid_count += 1\n",
    "    \n",
    "    if writer: writer.close()\n",
    "    print(f\"‚úÖ {prefix} Done. Wrote {valid_count} valid images (Skipped {len(data_to_process) - valid_count} bad files).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìù STEP 3: EXECUTE\n",
    "# =============================================================================\n",
    "\n",
    "if MODE != 'CANCEL':\n",
    "    SAVE_DIR = os.path.join(OUTPUT_BASE_DIR, RUN_TIMESTAMP)\n",
    "    \n",
    "    if MODE == 'OVERWRITE':\n",
    "        print(f\"üóëÔ∏è Deleting old data in {SAVE_DIR}...\")\n",
    "        shutil.rmtree(SAVE_DIR)\n",
    "    \n",
    "    # 1. File Scanning\n",
    "    print(\"üîç Scanning files...\")\n",
    "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "    class_map = {name: i for i, name in enumerate(classes)}\n",
    "    \n",
    "    all_files = []\n",
    "    valid_ext = {'.jpg', '.jpeg', '.png', '.bmp', '.webp', '.heic', '.heif'}\n",
    "    \n",
    "    skipped_bark = 0\n",
    "    \n",
    "    for cls in tqdm(classes):\n",
    "        cls_path = os.path.join(DATA_DIR, cls)\n",
    "        if os.path.exists(cls_path):\n",
    "            for f in os.listdir(cls_path):\n",
    "                if os.path.splitext(f)[1].lower() in valid_ext:\n",
    "                    if 'bark01' in f.lower():\n",
    "                        skipped_bark += 1\n",
    "                    else:\n",
    "                        all_files.append(os.path.join(cls_path, f))\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Skipped {skipped_bark} 'bark01' images.\")\n",
    "    \n",
    "    # 2. Shuffle & Split\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(all_files)\n",
    "    val_count = int(len(all_files) * VAL_SPLIT)\n",
    "    train_files = all_files[val_count:]\n",
    "    val_files = all_files[:val_count]\n",
    "    \n",
    "    print(f\"üìä Total Candidates: {len(all_files)} (Train: {len(train_files)}, Val: {len(val_files)})\")\n",
    "    \n",
    "    # 3. Process with Validation\n",
    "    is_resume = (MODE == 'RESUME')\n",
    "    write_tfrecords_robust(train_files, os.path.join(SAVE_DIR, 'train'), 'train_data', class_map, resume=is_resume)\n",
    "    write_tfrecords_robust(val_files, os.path.join(SAVE_DIR, 'val'), 'val_data', class_map, resume=is_resume)\n",
    "    \n",
    "    print(f\"\\nüéâ READY FOR TRAINING. ID: {RUN_TIMESTAMP}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
